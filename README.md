In this paper, we propose a model-based reinforcement learning method that can improve the uncertainty for policy in multitask reinforcement learning without sampling from the model. Most of model-free reinforcement learning agent use only the observation of the environment to make decisions. This cause uncertainty when they encounter a new environment or the environment change constantly. We introduce a method that use a latent variable as a prediction of the environment, and this variable is able to support an agent that can make certain decisions in the next state of the environment. In practice, it show that the policy depend on our latent to give action. This increases the stability of agent but still hold efficiency in maximizing return in fast changing environment..
